---
id: tutorials-jobs-scripts-delete-script
title: Delete Job Script
sidebar_label: Delete Script
slug: /tutorials/jobs/scripts/delete-script
---

Managing job script lifecycle includes knowing when and how to properly delete scripts that are no longer needed. This guide covers safe deletion practices, cleanup procedures, and script retirement strategies.

## When to Delete Scripts

Consider deleting scripts in these scenarios:

### Obsolete Scripts

- **Deprecated Workflows**: Scripts for processes that have been replaced or discontinued
- **Outdated Software**: Scripts using software versions no longer supported
- **Legacy Methods**: Old approaches superseded by more efficient or accurate methods
- **Experimental Code**: Prototype scripts that didn't lead to production use

### Duplicate or Redundant Scripts

- **Functionality Overlap**: Multiple scripts performing similar tasks
- **Replaced Versions**: Older versions when newer versions are stable
- **Test Scripts**: Development and testing scripts no longer needed
- **Personal Copies**: Individual copies of scripts now available in shared repositories

### Security and Compliance Issues

- **Security Vulnerabilities**: Scripts with known security problems
- **Policy Violations**: Scripts that violate updated organizational policies
- **Exposed Credentials**: Scripts accidentally containing sensitive information
- **Licensing Issues**: Scripts using software without proper licensing

### Resource and Maintenance Burden

- **Unused Scripts**: Scripts with no usage over extended periods
- **High Maintenance**: Scripts requiring excessive maintenance effort
- **Resource Intensive**: Scripts consuming disproportionate storage or compute resources
- **Support Burden**: Scripts generating frequent support requests

## Pre-Deletion Assessment

Before deleting scripts, perform thorough assessment:

### Usage Analysis

Check script usage patterns and dependencies:

```bash
#!/bin/bash
# Script usage analysis tool

analyze_script_usage() {
    local script_name="$1"
    local analysis_period="${2:-90}"  # days
    
    echo "Analyzing usage for script: $script_name"
    echo "Analysis period: $analysis_period days"
    echo "=================================="
    
    # Check job submission logs
    echo "Recent job submissions:"
    grep -r "$script_name" /var/log/slurm/ | \
        awk -v date="$(date -d "-${analysis_period} days" +%Y-%m-%d)" \
        '$0 > date' | wc -l
    
    # Check file access logs
    echo "Recent file accesses:"
    find /path/to/scripts -name "$script_name" -exec stat {} \; | \
        grep "Access:" | head -5
    
    # Check version control history
    echo "Recent commits:"
    git log --oneline --since="${analysis_period} days ago" -- "$script_name"
    
    # Check dependencies
    echo "Dependencies on this script:"
    grep -r "$script_name" /path/to/all/scripts/ | \
        grep -v "$script_name:" | head -10
}

# Example usage
analyze_script_usage "old_analysis.py" 180
```

### Dependency Mapping

Identify scripts and workflows that depend on the target script:

```python
#!/usr/bin/env python3
"""
Script dependency analyzer
"""

import os
import re
import sys
from pathlib import Path
from collections import defaultdict

class DependencyAnalyzer:
    def __init__(self, scripts_directory):
        self.scripts_dir = Path(scripts_directory)
        self.dependencies = defaultdict(list)
        self.reverse_dependencies = defaultdict(list)
    
    def analyze_dependencies(self):
        """Analyze dependencies across all scripts"""
        script_files = []
        
        # Find all script files
        for ext in ['*.py', '*.sh', '*.R', '*.pl']:
            script_files.extend(self.scripts_dir.rglob(ext))
        
        print(f"Analyzing {len(script_files)} script files...")
        
        for script_file in script_files:
            self.analyze_single_script(script_file)
    
    def analyze_single_script(self, script_file):
        """Analyze dependencies for a single script"""
        try:
            with open(script_file, 'r') as f:
                content = f.read()
            
            # Look for various dependency patterns
            patterns = [
                r'source\s+([^\s]+)',  # bash source
                r'import\s+([^\s;]+)',  # python import
                r'from\s+([^\s]+)\s+import',  # python from import
                r'library\(([^)]+)\)',  # R library
                r'require\(([^)]+)\)',  # R require
                r'sbatch\s+([^\s]+)',  # job submission
                r'bash\s+([^\s]+)',  # bash execution
                r'python\s+([^\s]+)',  # python execution
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    dependency = match.strip('\'"')
                    if self.is_local_script(dependency):
                        self.dependencies[str(script_file)].append(dependency)
                        self.reverse_dependencies[dependency].append(str(script_file))
        
        except Exception as e:
            print(f"Error analyzing {script_file}: {e}")
    
    def is_local_script(self, path):
        """Check if dependency is a local script"""
        return (Path(path).suffix in ['.py', '.sh', '.R', '.pl'] and
                not path.startswith('/usr/') and
                not path.startswith('/bin/'))
    
    def get_dependencies(self, script_name):
        """Get direct dependencies of a script"""
        return self.dependencies.get(script_name, [])
    
    def get_dependents(self, script_name):
        """Get scripts that depend on this script"""
        return self.reverse_dependencies.get(script_name, [])
    
    def generate_report(self, target_script):
        """Generate dependency report for target script"""
        print(f"\nDependency Analysis for: {target_script}")
        print("=" * 50)
        
        # Scripts this script depends on
        deps = self.get_dependencies(target_script)
        print(f"\nDirect Dependencies ({len(deps)}):")
        for dep in deps:
            print(f"  - {dep}")
        
        # Scripts that depend on this script
        dependents = self.get_dependents(target_script)
        print(f"\nScripts Depending on This ({len(dependents)}):")
        for dependent in dependents:
            print(f"  - {dependent}")
        
        # Risk assessment
        if dependents:
            print(f"\n⚠️  WARNING: {len(dependents)} scripts depend on this script!")
            print("   Deletion will break these dependent scripts.")
        else:
            print(f"\n✅ SAFE: No scripts depend on {target_script}")

# Usage example
if __name__ == "__main__":
    analyzer = DependencyAnalyzer("/path/to/scripts")
    analyzer.analyze_dependencies()
    
    if len(sys.argv) > 1:
        analyzer.generate_report(sys.argv[1])
```

### Impact Assessment

Evaluate the impact of script deletion:

```bash
#!/bin/bash
# Impact assessment for script deletion

assess_deletion_impact() {
    local script_path="$1"
    local script_name=$(basename "$script_path")
    
    echo "Impact Assessment for: $script_name"
    echo "=================================="
    
    # Check active jobs using this script
    echo "Active jobs using this script:"
    squeue -u all --format="%.18i %.9P %.20j %.8u %.8T %.10M %.6D %R" | \
        grep "$script_name" || echo "None found"
    
    # Check scheduled jobs
    echo -e "\nScheduled jobs (next 7 days):"
    # This would query your job scheduler for future jobs
    # Implementation depends on your scheduling system
    
    # Check user projects and workflows
    echo -e "\nProjects using this script:"
    find /projects -name "*.yaml" -o -name "*.json" -o -name "*.config" | \
        xargs grep -l "$script_name" 2>/dev/null | head -10
    
    # Check documentation references
    echo -e "\nDocumentation references:"
    find /docs -name "*.md" -o -name "*.rst" | \
        xargs grep -l "$script_name" 2>/dev/null | head -5
    
    # Estimate affected users
    echo -e "\nPotential affected users:"
    grep -r "$script_name" /home/*/jobs/ 2>/dev/null | \
        cut -d: -f1 | cut -d/ -f3 | sort -u | wc -l
}

# User notification check
check_notification_needed() {
    local script_name="$1"
    local affected_users="$2"
    
    if [[ $affected_users -gt 0 ]]; then
        echo -e "\n⚠️  NOTIFICATION REQUIRED"
        echo "   Affected users: $affected_users"
        echo "   Recommended notice period: 30-60 days"
        echo "   Communication channels:"
        echo "   - Email to affected users"
        echo "   - Platform announcements"
        echo "   - Documentation updates"
    else
        echo -e "\n✅ NO NOTIFICATION REQUIRED"
        echo "   No active users detected"
    fi
}
```

## Safe Deletion Procedures

### Soft Deletion (Recommended)

Implement soft deletion to allow recovery:

```bash
#!/bin/bash
# Soft deletion implementation

ARCHIVE_DIR="/archive/deleted_scripts"
RETENTION_PERIOD=90  # days

soft_delete_script() {
    local script_path="$1"
    local deletion_reason="$2"
    local script_name=$(basename "$script_path")
    local timestamp=$(date +%Y%m%d_%H%M%S)
    
    # Create archive directory
    mkdir -p "$ARCHIVE_DIR"
    
    # Create deletion metadata
    cat > "$ARCHIVE_DIR/${script_name}_${timestamp}.metadata" << EOF
Original Path: $script_path
Deletion Date: $(date)
Deleted By: $USER
Reason: $deletion_reason
Retention Until: $(date -d "+$RETENTION_PERIOD days")
EOF
    
    # Archive the script
    if [[ -f "$script_path" ]]; then
        cp "$script_path" "$ARCHIVE_DIR/${script_name}_${timestamp}.archived"
        echo "Script archived to: $ARCHIVE_DIR/${script_name}_${timestamp}.archived"
    fi
    
    # Mark as deleted (rename with .deleted suffix)
    mv "$script_path" "${script_path}.deleted"
    echo "Script soft-deleted: ${script_path}.deleted"
    
    # Log the deletion
    echo "$(date): Soft deleted $script_path (reason: $deletion_reason)" >> \
        "$ARCHIVE_DIR/deletion_log.txt"
}

# Example usage
soft_delete_script "/scripts/old_analysis.py" "Replaced by new_analysis_v2.py"
```

### Hard Deletion

Permanent removal when appropriate:

```bash
#!/bin/bash
# Hard deletion with safety checks

hard_delete_script() {
    local script_path="$1"
    local force_delete="$2"
    
    # Safety checks
    if [[ ! "$force_delete" == "FORCE" ]]; then
        echo "Hard deletion requires FORCE confirmation"
        echo "Usage: hard_delete_script <path> FORCE"
        return 1
    fi
    
    # Final confirmation
    echo "WARNING: This will permanently delete $script_path"
    echo "This action cannot be undone!"
    read -p "Type 'DELETE' to confirm: " confirmation
    
    if [[ "$confirmation" != "DELETE" ]]; then
        echo "Deletion cancelled"
        return 1
    fi
    
    # Create final backup
    local backup_name="final_backup_$(basename "$script_path")_$(date +%Y%m%d)"
    cp "$script_path" "/tmp/$backup_name" 2>/dev/null
    
    # Remove the script
    rm -f "$script_path"
    
    if [[ $? -eq 0 ]]; then
        echo "Script permanently deleted: $script_path"
        echo "Final backup available: /tmp/$backup_name"
        
        # Log the hard deletion
        echo "$(date): Hard deleted $script_path by $USER" >> \
            "/var/log/script_deletions.log"
    else
        echo "Failed to delete script: $script_path"
        return 1
    fi
}
```

### Batch Deletion

Delete multiple scripts based on criteria:

```python
#!/usr/bin/env python3
"""
Batch script deletion utility
"""

import os
import shutil
import json
from datetime import datetime, timedelta
from pathlib import Path

class BatchDeleter:
    def __init__(self, base_directory, dry_run=True):
        self.base_dir = Path(base_directory)
        self.dry_run = dry_run
        self.archive_dir = Path("/archive/batch_deletions")
        self.deletion_log = []
    
    def find_scripts_by_criteria(self, criteria):
        """Find scripts matching deletion criteria"""
        matching_scripts = []
        
        for script_file in self.base_dir.rglob("*"):
            if not script_file.is_file():
                continue
                
            if script_file.suffix not in ['.py', '.sh', '.R', '.pl']:
                continue
            
            if self.matches_criteria(script_file, criteria):
                matching_scripts.append(script_file)
        
        return matching_scripts
    
    def matches_criteria(self, script_file, criteria):
        """Check if script matches deletion criteria"""
        stat_info = script_file.stat()
        
        # Check last modification time
        if 'days_since_modified' in criteria:
            days_ago = datetime.now() - datetime.fromtimestamp(stat_info.st_mtime)
            if days_ago.days < criteria['days_since_modified']:
                return False
        
        # Check file size
        if 'max_size_kb' in criteria:
            if stat_info.st_size > criteria['max_size_kb'] * 1024:
                return False
        
        # Check name patterns
        if 'name_patterns' in criteria:
            for pattern in criteria['name_patterns']:
                if pattern in script_file.name:
                    return True
            return False
        
        # Check for test files
        if criteria.get('test_files_only', False):
            test_indicators = ['test_', '_test', 'temp_', '_temp', 'debug_']
            return any(indicator in script_file.name.lower() 
                      for indicator in test_indicators)
        
        return True
    
    def delete_scripts(self, scripts_to_delete, reason="Batch deletion"):
        """Delete list of scripts"""
        if not scripts_to_delete:
            print("No scripts found matching criteria")
            return
        
        print(f"{'DRY RUN: ' if self.dry_run else ''}Deleting {len(scripts_to_delete)} scripts")
        
        # Create archive directory
        if not self.dry_run:
            self.archive_dir.mkdir(parents=True, exist_ok=True)
        
        for script_file in scripts_to_delete:
            self.delete_single_script(script_file, reason)
        
        # Save deletion log
        if not self.dry_run:
            log_file = self.archive_dir / f"deletion_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(log_file, 'w') as f:
                json.dump(self.deletion_log, f, indent=2, default=str)
    
    def delete_single_script(self, script_file, reason):
        """Delete a single script with archiving"""
        deletion_record = {
            'script_path': str(script_file),
            'deletion_time': datetime.now(),
            'reason': reason,
            'file_size': script_file.stat().st_size,
            'last_modified': datetime.fromtimestamp(script_file.stat().st_mtime)
        }
        
        print(f"{'[DRY RUN] ' if self.dry_run else ''}Deleting: {script_file}")
        
        if not self.dry_run:
            # Archive before deletion
            archive_name = f"{script_file.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            archive_path = self.archive_dir / archive_name
            
            try:
                shutil.copy2(script_file, archive_path)
                script_file.unlink()
                deletion_record['archived_to'] = str(archive_path)
                deletion_record['status'] = 'success'
            except Exception as e:
                deletion_record['status'] = 'failed'
                deletion_record['error'] = str(e)
                print(f"Error deleting {script_file}: {e}")
        
        self.deletion_log.append(deletion_record)

# Usage example
def main():
    # Define deletion criteria
    cleanup_criteria = {
        'days_since_modified': 180,  # Not modified in 6 months
        'test_files_only': True,     # Only test/temp files
        'name_patterns': ['test_', 'temp_', 'debug_']
    }
    
    # Initialize deleter (dry run first)
    deleter = BatchDeleter("/path/to/scripts", dry_run=True)
    
    # Find matching scripts
    scripts_to_delete = deleter.find_scripts_by_criteria(cleanup_criteria)
    
    print(f"Found {len(scripts_to_delete)} scripts matching criteria:")
    for script in scripts_to_delete[:10]:  # Show first 10
        print(f"  - {script}")
    
    if len(scripts_to_delete) > 10:
        print(f"  ... and {len(scripts_to_delete) - 10} more")
    
    # Confirm and execute deletion
    if input("Proceed with deletion? (yes/no): ").lower() == 'yes':
        deleter.dry_run = False
        deleter.delete_scripts(scripts_to_delete, "Automated cleanup")

if __name__ == "__main__":
    main()
```

## User Notification and Communication

### Notification Strategy

Communicate script deletions effectively:

```bash
#!/bin/bash
# User notification system

notify_users() {
    local script_name="$1"
    local deletion_date="$2"
    local replacement_script="$3"
    
    # Find affected users
    affected_users=$(grep -r "$script_name" /home/*/jobs/ 2>/dev/null | \
                    cut -d: -f1 | cut -d/ -f3 | sort -u)
    
    # Create notification message
    cat > notification_message.txt << EOF
Subject: Important: Script Deletion Notice - $script_name

Dear Vantage Users,

This is to inform you that the script "$script_name" has been scheduled for deletion.

Deletion Details:
- Script: $script_name
- Deletion Date: $deletion_date
- Reason: Replaced by improved version

Action Required:
$(if [[ -n "$replacement_script" ]]; then
    echo "- Update your workflows to use: $replacement_script"
    echo "- Review migration guide: /docs/migration/$script_name"
else
    echo "- Remove references to this script from your workflows"
    echo "- Contact support if you need assistance finding alternatives"
fi)

Timeline:
- Notice Period: 30 days
- Soft Deletion: $deletion_date
- Final Removal: $(date -d "$deletion_date +90 days")

Support:
If you have questions or need assistance, please contact:
- Support Email: platform-support@organization.org
- Documentation: /docs/script-migration
- Office Hours: Tuesdays 2-4 PM

Best regards,
Platform Team
EOF
    
    # Send notifications
    for user in $affected_users; do
        echo "Notifying user: $user"
        # Send email notification
        mail -s "Script Deletion Notice: $script_name" "$user@organization.org" \
             < notification_message.txt
        
        # Create user notification file
        mkdir -p "/home/$user/.vantage_notifications"
        cp notification_message.txt "/home/$user/.vantage_notifications/script_deletion_$script_name.txt"
    done
    
    # Post platform-wide announcement
    echo "Posting platform announcement..."
    # Implementation depends on your announcement system
}
```

### Migration Assistance

Provide migration support for affected users:

```python
#!/usr/bin/env python3
"""
Script migration assistant
"""

class MigrationAssistant:
    def __init__(self, old_script, new_script):
        self.old_script = old_script
        self.new_script = new_script
        self.migration_guide = {}
    
    def generate_migration_guide(self):
        """Generate migration guide for users"""
        guide = f"""
# Migration Guide: OLD_SCRIPT → NEW_SCRIPT

## Overview
The script `OLD_SCRIPT` has been replaced by `NEW_SCRIPT` 
with improved performance, features, and maintainability.

## Key Differences

### Command Line Interface
```bash
# Old usage
OLD_SCRIPT input.data output.results

# New usage  
NEW_SCRIPT --input input.data --output output.results
```

### Configuration Changes
- Configuration files now use YAML format instead of INI
- Some parameter names have changed for clarity
- New optional parameters for enhanced functionality

### Output Format
- Results are now saved in JSON format for better interoperability
- Additional metadata is included in output files
- Backward compatibility mode available with --legacy-output flag

## Migration Steps

### Step 1: Update Job Scripts
Replace old script calls with new syntax:

```bash
# Find and replace in your job scripts
sed -i 's/OLD_SCRIPT/NEW_SCRIPT/g' your_job_script.sh

# Update parameter format
sed -i 's/old_parameter/--new-parameter/g' your_job_script.sh
```

### Step 2: Convert Configuration Files
Use the provided conversion utility:

```bash
python convert_config.py old_config.ini new_config.yaml
```

### Step 3: Update Workflows
Review and update any workflow dependencies:
- Check pipeline scripts for references
- Update documentation and README files
- Verify integration with other tools

### Step 4: Test Migration
Run validation tests to ensure compatibility:

```bash
# Test with sample data
NEW_SCRIPT --input sample_data.txt --output test_results/ --validate

# Compare results with legacy version
diff old_results/ test_results/
```

## Troubleshooting

### Common Issues
1. **Parameter not recognized**: Check new parameter names in documentation
2. **Output format different**: Use --legacy-output for backward compatibility
3. **Performance slower**: Adjust thread count with --threads parameter

### Getting Help
- Documentation: /docs/scripts/NEW_SCRIPT
- Examples: /examples/NEW_SCRIPT/
- Support: platform-support@organization.org

## Timeline
- Migration Period: 30 days from notice
- Legacy Script Removal: OLD_SCRIPT will be removed on [DATE]
- Support Cutoff: Support for OLD_SCRIPT ends on [DATE]
"""
        return guide
    
    def create_conversion_script(self):
        """Create automated conversion script"""
        conversion_script = f"""#!/bin/bash
# Automatic migration script for OLD_SCRIPT → NEW_SCRIPT

echo "Starting migration from OLD_SCRIPT to NEW_SCRIPT"

# Find all job scripts using old script
job_scripts=$(grep -r "OLD_SCRIPT" ~/jobs/ | cut -d: -f1 | sort -u)

for script in $job_scripts; do
    echo "Updating: $script"
    
    # Create backup
    cp "$script" "${{script}}.backup"
    
    # Perform replacements
    sed -i 's/OLD_SCRIPT/NEW_SCRIPT/g' "$script"
    sed -i 's/old_param/--new-param/g' "$script"
    
    echo "Updated: $script (backup: ${{script}}.backup)"
done

echo "Migration completed. Please test your updated scripts."
echo "Backups are available with .backup extension."
"""
        return conversion_script
```

## Recovery Procedures

### Soft Delete Recovery

Restore accidentally deleted scripts:

```bash
#!/bin/bash
# Script recovery utility

recover_deleted_script() {
    local script_name="$1"
    local target_path="$2"
    
    echo "Searching for deleted script: $script_name"
    
    # Find archived versions
    archived_versions=$(find "$ARCHIVE_DIR" -name "${script_name}_*.archived" | sort -r)
    
    if [[ -z "$archived_versions" ]]; then
        echo "No archived versions found for $script_name"
        return 1
    fi
    
    echo "Available archived versions:"
    select version in $archived_versions "Cancel"; do
        case $version in
            "Cancel")
                echo "Recovery cancelled"
                return 0
                ;;
            *)
                if [[ -f "$version" ]]; then
                    echo "Recovering from: $version"
                    
                    # Get metadata
                    metadata_file="${version%.archived}.metadata"
                    if [[ -f "$metadata_file" ]]; then
                        echo "Original metadata:"
                        cat "$metadata_file"
                        echo ""
                    fi
                    
                    # Confirm recovery location
                    if [[ -z "$target_path" ]]; then
                        echo "Enter recovery path (or press Enter for original location):"
                        read target_path
                        
                        if [[ -z "$target_path" ]]; then
                            target_path=$(grep "Original Path:" "$metadata_file" | cut -d: -f2- | xargs)
                        fi
                    fi
                    
                    # Perform recovery
                    cp "$version" "$target_path"
                    echo "Script recovered to: $target_path"
                    
                    # Log recovery
                    echo "$(date): Recovered $script_name from $version to $target_path" >> \
                        "$ARCHIVE_DIR/recovery_log.txt"
                    
                    return 0
                else
                    echo "Invalid selection"
                fi
                ;;
        esac
    done
}

# Example usage
recover_deleted_script "analysis_script.py"
```

### Backup Recovery

Restore from system backups:

```bash
#!/bin/bash
# Backup-based recovery

restore_from_backup() {
    local script_path="$1"
    local backup_date="$2"
    
    echo "Attempting backup recovery for: $script_path"
    echo "Target date: $backup_date"
    
    # This would interface with your backup system
    # Implementation depends on backup infrastructure
    
    backup_file="/backups/${backup_date}/scripts$(dirname "$script_path")/$(basename "$script_path")"
    
    if [[ -f "$backup_file" ]]; then
        echo "Found backup: $backup_file"
        
        # Verify backup integrity
        if file "$backup_file" | grep -q "text"; then
            echo "Backup verification successful"
            
            # Restore file
            cp "$backup_file" "$script_path"
            echo "Script restored from backup"
            
            # Set appropriate permissions
            chmod +x "$script_path"
            
            return 0
        else
            echo "Backup file appears corrupted"
            return 1
        fi
    else
        echo "No backup found for specified date"
        return 1
    fi
}
```

## Cleanup and Maintenance

### Automated Cleanup

Automate cleanup of old archives and logs:

```bash
#!/bin/bash
# Automated cleanup of deletion artifacts

cleanup_deletion_artifacts() {
    local retention_days="${1:-180}"  # Default 6 months retention
    
    echo "Starting cleanup of deletion artifacts older than $retention_days days"
    
    # Clean up archived scripts
    echo "Cleaning archived scripts..."
    find "$ARCHIVE_DIR" -name "*.archived" -mtime +$retention_days -delete
    
    # Clean up metadata files
    echo "Cleaning metadata files..."
    find "$ARCHIVE_DIR" -name "*.metadata" -mtime +$retention_days -delete
    
    # Compress old logs
    echo "Compressing old logs..."
    find "$ARCHIVE_DIR" -name "*.log" -mtime +30 -exec gzip {} \;
    
    # Remove compressed logs older than retention period
    find "$ARCHIVE_DIR" -name "*.log.gz" -mtime +$retention_days -delete
    
    # Clean up temporary files
    echo "Cleaning temporary files..."
    find /tmp -name "*_backup_*" -mtime +7 -delete
    
    echo "Cleanup completed"
}

# Schedule regular cleanup (add to crontab)
# 0 2 * * 0 /path/to/cleanup_deletion_artifacts.sh >/dev/null 2>&1
```

### Deletion Audit

Generate audit reports for deletion activities:

```python
#!/usr/bin/env python3
"""
Deletion audit and reporting
"""

import json
import csv
from datetime import datetime, timedelta
from pathlib import Path

class DeletionAuditor:
    def __init__(self, archive_dir="/archive/deleted_scripts"):
        self.archive_dir = Path(archive_dir)
    
    def generate_audit_report(self, days=30):
        """Generate audit report for recent deletions"""
        since_date = datetime.now() - timedelta(days=days)
        
        # Collect deletion records
        deletion_logs = list(self.archive_dir.glob("deletion_log_*.json"))
        all_deletions = []
        
        for log_file in deletion_logs:
            try:
                with open(log_file, 'r') as f:
                    log_data = json.load(f)
                    for record in log_data:
                        deletion_date = datetime.fromisoformat(record['deletion_time'])
                        if deletion_date >= since_date:
                            all_deletions.append(record)
            except Exception as e:
                print(f"Error reading {log_file}: {e}")
        
        # Generate report
        report = f"Deletion Audit Report (Last {days} days)\n"
        report += "=" * 50 + "\n\n"
        
        if not all_deletions:
            report += "No deletions found in the specified period.\n"
            return report
        
        # Summary statistics
        total_deletions = len(all_deletions)
        successful_deletions = sum(1 for d in all_deletions if d.get('status') == 'success')
        total_size = sum(d.get('file_size', 0) for d in all_deletions)
        
        report += f"Summary:\n"
        report += f"  Total Deletions: {total_deletions}\n"
        report += f"  Successful: {successful_deletions}\n"
        report += f"  Failed: {total_deletions - successful_deletions}\n"
        report += f"  Total Size Freed: {total_size / 1024 / 1024:.2f} MB\n\n"
        
        # Deletion reasons breakdown
        reasons = {}
        for deletion in all_deletions:
            reason = deletion.get('reason', 'Unknown')
            reasons[reason] = reasons.get(reason, 0) + 1
        
        report += "Deletion Reasons:\n"
        for reason, count in sorted(reasons.items(), key=lambda x: x[1], reverse=True):
            report += f"  {reason}: {count}\n"
        
        return report
    
    def export_deletion_data(self, output_file="deletion_audit.csv"):
        """Export deletion data to CSV for analysis"""
        deletion_logs = list(self.archive_dir.glob("deletion_log_*.json"))
        all_deletions = []
        
        for log_file in deletion_logs:
            try:
                with open(log_file, 'r') as f:
                    log_data = json.load(f)
                    all_deletions.extend(log_data)
            except Exception as e:
                print(f"Error reading {log_file}: {e}")
        
        # Write to CSV
        with open(output_file, 'w', newline='') as csvfile:
            if all_deletions:
                fieldnames = all_deletions[0].keys()
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_deletions)
        
        print(f"Deletion data exported to: {output_file}")
```

## Best Practices Summary

### Planning and Assessment

- Conduct thorough usage analysis before deletion
- Map dependencies and potential impacts
- Provide adequate notice to affected users
- Offer migration assistance and alternatives

### Safety and Recovery

- Use soft deletion as default approach
- Maintain archives with appropriate retention periods
- Document deletion reasons and decisions
- Implement recovery procedures for accidental deletions

### Communication and Support

- Notify users well in advance of deletions
- Provide clear migration guides and alternatives
- Offer support during transition periods
- Document lessons learned for future deletions

### Automation and Maintenance

- Automate routine cleanup tasks
- Generate regular audit reports
- Monitor deletion impact and success rates
- Maintain deletion policies and procedures

## Next Steps

After implementing script deletion procedures:

- Establish regular review cycles for script lifecycle management
- Train team members on deletion procedures and best practices
- Monitor and optimize deletion workflows based on experience
- Consider implementing automated script lifecycle policies
- Document organizational standards for script retirement
